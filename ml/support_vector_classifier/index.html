<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Support Vector Classifier - ML LAB</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Support Vector Classifier" />
<meta property="og:description" content="Support Vector Classifier는 Optimal Seperating Hyperplane을 Nonseperable Case에 대해 일반화한 모형이다. Support Vector Classifier 역시 Margin을 최대화하는 방향으로 작동하지만, 일정 수준의 오분류를 허용함으로써 Nonseperable Case에서도 수렴할 수 있다는 것이 차이점이다.
일정 수준의 오분류를 허용한다는 것을 &lsquo;Slack Variable&rsquo;라고 불리는 $\xi_i$를 사용하여 다음과 같이 표현할 수 있다. (위 그림에서 $\xi_i^*=M\xi_i$이다.)
$\displaystyle \max_{\boldsymbol{\beta}, \beta_0, \Vert\boldsymbol{\beta}\Vert=1}M \quad\text{subject to}\quad y_i(\mathbf{x}_i^T\boldsymbol{\beta}&#43;\beta_0)\geq M-\xi_i,\; \xi_i\geq 0,\; \sum_{i=1}^N\xi_i\leq c,\; ^\forall i$
하지만 이러한 형태의 제약 조건을 사용할 경우, 더 이상 주어진 문제가 Convex Optimization에 속하지 않는다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://statkwon.github.io/ml/support_vector_classifier/" /><meta property="article:section" content="ml" />
<meta property="article:published_time" content="2022-01-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-01-23T00:00:00+00:00" />

		<meta itemprop="name" content="Support Vector Classifier">
<meta itemprop="description" content="Support Vector Classifier는 Optimal Seperating Hyperplane을 Nonseperable Case에 대해 일반화한 모형이다. Support Vector Classifier 역시 Margin을 최대화하는 방향으로 작동하지만, 일정 수준의 오분류를 허용함으로써 Nonseperable Case에서도 수렴할 수 있다는 것이 차이점이다.
일정 수준의 오분류를 허용한다는 것을 &lsquo;Slack Variable&rsquo;라고 불리는 $\xi_i$를 사용하여 다음과 같이 표현할 수 있다. (위 그림에서 $\xi_i^*=M\xi_i$이다.)
$\displaystyle \max_{\boldsymbol{\beta}, \beta_0, \Vert\boldsymbol{\beta}\Vert=1}M \quad\text{subject to}\quad y_i(\mathbf{x}_i^T\boldsymbol{\beta}&#43;\beta_0)\geq M-\xi_i,\; \xi_i\geq 0,\; \sum_{i=1}^N\xi_i\leq c,\; ^\forall i$
하지만 이러한 형태의 제약 조건을 사용할 경우, 더 이상 주어진 문제가 Convex Optimization에 속하지 않는다."><meta itemprop="datePublished" content="2022-01-23T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-01-23T00:00:00+00:00" />
<meta itemprop="wordCount" content="356">
<meta itemprop="keywords" content="SVM," />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="INTUITIVE STATISTICAL LEARNING" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">INTUITIVE STATISTICAL LEARNING</div>
					<div class="logo__tagline">Wanna be a Skillful Data Scientist</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Support Vector Classifier</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2022-01-23T00:00:00Z">2022-01-23</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/ml/" rel="category">ML</a>
	</span>
</div></div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#process-for-solving-dual-problem">Process for Solving Dual Problem</a></li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<p>Support Vector Classifier는 Optimal Seperating Hyperplane을 Nonseperable Case에 대해 일반화한 모형이다. Support Vector Classifier 역시 Margin을 최대화하는 방향으로 작동하지만, 일정 수준의 오분류를 허용함으로써 Nonseperable Case에서도 수렴할 수 있다는 것이 차이점이다.</p>
<figure><img src="/ml/svc1.png" width="400"/>
</figure>

<p>일정 수준의 오분류를 허용한다는 것을 &lsquo;Slack Variable&rsquo;라고 불리는 $\xi_i$를 사용하여 다음과 같이 표현할 수 있다. (위 그림에서 $\xi_i^*=M\xi_i$이다.)</p>
<p>$\displaystyle \max_{\boldsymbol{\beta}, \beta_0, \Vert\boldsymbol{\beta}\Vert=1}M \quad\text{subject to}\quad y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)\geq M-\xi_i,\; \xi_i\geq 0,\; \sum_{i=1}^N\xi_i\leq c,\; ^\forall i$</p>
<p>하지만 이러한 형태의 제약 조건을 사용할 경우, 더 이상 주어진 문제가 Convex Optimization에 속하지 않는다. 따라서 직관성은 다소 떨어지더라도 Convex Optimization의 조건을 충족시키기 위하여 다음과 같은 형태의 제약식을 사용한다.</p>
<p>$\displaystyle \max_{\boldsymbol{\beta}, \beta_0, \Vert\boldsymbol{\beta}\Vert=1}M \quad\text{subject to}\quad y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)\geq M(1-\xi_i),\; \xi_i\geq 0,\; \sum_{i=1}^N\xi_i\leq c,\; ^\forall i$</p>
<p>이때 $\xi_i$는 $\mathbf{x}_i$가 잘못 위치한 정도에 비례하는 값이라고 할 수 있다. $\mathbf{x}_i$가 오분류된 경우 $\xi_i$는 $1$보다 큰 값을 갖는다. 따라서 $\sum\xi_i\leq c$의 상한을 두는 것은 오분류되는 데이터의 최대 개수를 $c$개로 제한하는 것으로 해석할 수 있다.</p>
<p><a href="/ml/optimal_seperating_hyperplanes">Optimal Seperating Hyperplanes</a>에서 그랬던 것처럼, $M=1/\Vert\boldsymbol{\beta}\Vert$로 둠으로써 다음과 같은 형태로 문제를 변형하고, Dual Problem을 풀어 최적해를 구할 수 있다.</p>
<p>$\displaystyle \Leftrightarrow \min_{\boldsymbol{\beta}, \beta_0}\dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2 \quad\text{subject to}\quad y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)\geq 1-\xi_i,\; \xi_i\geq 0,\; \sum_{i=1}^N\xi_i\leq c,\; ^\forall i$ <br>
$\displaystyle \Leftrightarrow \min_{\boldsymbol{\beta}, \beta_0}\dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2+c\sum_{i=1}^N\xi_i \quad\text{subject to}\quad y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)≥1-\xi_i,\; \xi_i≥0,\; ^\forall i$</p>
<p>Optimal Seperating Hyperplane과 마찬가지로 $\hat{G}(\mathbf{x})=\text{sign}(\hat{\beta}_0+\hat{\boldsymbol{\beta}}^T\mathbf{x})$로 새로운 데이터를 분류한다. 최적의 $c$는 Cross-Validation으로 결정한다.</p>
<h2 id="process-for-solving-dual-problem">Process for Solving Dual Problem</h2>
<p>Lagrangian Primal Function: $\displaystyle l(\boldsymbol{\beta}, \beta_0, \boldsymbol{\xi}, \boldsymbol{\lambda}, \boldsymbol{\mu})=\dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2+c\sum_{i=1}^N\xi_i-\sum_{i=1}^N\lambda_i\{y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)-(1-\xi_i)\}-\sum_{i=1}^N\mu_i\xi_i$</p>
<p>$\displaystyle \dfrac{\partial l}{\partial\boldsymbol{\beta}}=\boldsymbol{\beta}^T-\sum_{i=1}^N\lambda_iy_i\mathbf{x}_i^T=0 \quad\Rightarrow\quad \boldsymbol{\beta}^*=\sum_{i=1}^N\lambda_iy_i\mathbf{x}_i$ <br>
$\displaystyle \dfrac{\partial l}{\partial\beta_0}=-\sum_{i=1}^N\lambda_iy_i=0 \quad\Rightarrow\quad \sum_{i=1}^N\lambda_iy_i=0$ <br>
$\displaystyle \dfrac{\partial l}{\partial\boldsymbol{\xi}}=c\mathbf{1}^T-\boldsymbol{\lambda}^T-\boldsymbol{\mu}^T=0 \quad\Rightarrow\quad \boldsymbol{\lambda}=c\mathbf{1}-\boldsymbol{\mu}$</p>
<p>Lagrangian Dual Function: <br>
$\begin{aligned}
l(\boldsymbol{\beta}^*, \beta_0^*, \boldsymbol{\xi}^*, \boldsymbol{\lambda}, \boldsymbol{\mu})&amp;=\dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_j\mathbf{x}_i^T\mathbf{x}_j+c\sum_{i=1}^N\xi_i-\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_j\mathbf{x}_i^T\mathbf{x}_j-\sum_{i=1}^N\lambda_iy_i\beta_0+\sum_{i=1}^N\lambda_i-\sum_{i=1}^N\lambda_i\xi_i-\sum_{i=1}^N\mu_i\xi_i \\
&amp;=\sum_{i=1}^N\lambda_i-\dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_j\mathbf{x}_i^T\mathbf{x}_j
\end{aligned}$</p>
<p>Dual Problem: $\displaystyle \max_{\boldsymbol{\lambda}, \boldsymbol{\mu}}l(\boldsymbol{\beta}^*, \beta_0^*, \boldsymbol{\xi}^*, \boldsymbol{\lambda}, \boldsymbol{\mu}) \quad\text{subject to}\quad \mathbf{0}\leq\boldsymbol{\lambda}\leq c\mathbf{1},\; \mathbf{0}\leq\boldsymbol{\mu}\leq c\mathbf{1},\; \sum_{i=1}^N\lambda_iy_i=0$</p>
<p>최적해 $\boldsymbol{\beta}^*$와 $\beta_0^*$는 KKT Condition인 $\lambda_i\{y_i(\mathbf{x}_i^T\boldsymbol{\beta})-(1-\xi_i)\}=0$과 $\mu_i\xi_i=0$을 만족시켜야 한다. 따라서 $\lambda_i&gt;0$인 경우 $y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)=1-\xi_i$이고, $\lambda_i=0$인 경우 $y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)&gt;1-\xi_i$이다. 이를 이용하여 앞서 구한 $\displaystyle \boldsymbol{\beta}^*=\sum_{i=1}^N\lambda_iy_i\mathbf{x}_i$를 $\displaystyle \boldsymbol{\beta}^*=\sum_{i:\lambda_i\neq0}\lambda_iy_i\mathbf{x}_i$로 나타낼 수 있다. 즉, Solution Vector $\boldsymbol{\beta}^*$는 $y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)=1-\xi_i$를 만족시키는 데이터들의 선형 결합으로 정의됨을 알 수 있다. 이러한 데이터를 Support Point(또는 Support Vector)라고 한다. $\beta_0^*$는 Support Point 중 Margin의 경계 위에 있는 데이터에 대해 $\lambda_i\{y_i(\mathbf{x}_i^T\boldsymbol{\beta})-(1-\xi_i)\}=0$을 풂으로써 구할 수 있다. 일반적으로는 값의 안정성을 위해 여러 개의 $\beta_0^*$를 구하여 평균을 내는 방식을 사용한다.</p>
<hr>
<p><strong>Reference</strong></p>
<ol>
<li>Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.</li>
</ol>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/svm/" rel="tag">SVM</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="STATKWON avatar" src="/img/avatar.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About STATKWON</span>
	</div>
	<div class="authorbox__description">
		Wanna be a Skillful Data Scientist
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/ml/optimal_seperating_hyperplanes/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Optimal Seperating Hyperplanes</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/ml/lda-dimension_reduction/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">LDA-Dimension Reduction</p>
		</a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statkwon" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 STATKWON.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\[','\]']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: { equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"] }
        }
    });
    MathJax.Hub.Queue(function() {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

</body>
</html>