<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Local Regression - ML LAB</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Local Regression" />
<meta property="og:description" content="Boundary issues of Nadaraya-Watson kernel regression can be solved by fitting a straight line rather than constants locally. This is the concept of local regression which fits a seperate weighted least squares at each target point $\mathbf{X}_0$. For convenience, let&rsquo;s consider the one-dimensional input space from now on.
Local Linear Regression $\displaystyle\underset{\boldsymbol{\beta}(x_0)}{\text{argmin}}(\mathbf{y}-X\boldsymbol{\beta}(x_0))^TW(x_0)(\mathbf{y}-X\boldsymbol{\beta}(x_0))$
The coefficients of local linear regression can be obtained by finding a optimal solution of the problem above, where $W$ is a diagonal matrix whose $i$th diagonal elements are $K_\lambda(x_0, x_i)$." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://statkwon.github.io/ml/local_regression/" /><meta property="article:section" content="ml" />
<meta property="article:published_time" content="2021-03-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-03-20T00:00:00+00:00" />

		<meta itemprop="name" content="Local Regression">
<meta itemprop="description" content="Boundary issues of Nadaraya-Watson kernel regression can be solved by fitting a straight line rather than constants locally. This is the concept of local regression which fits a seperate weighted least squares at each target point $\mathbf{X}_0$. For convenience, let&rsquo;s consider the one-dimensional input space from now on.
Local Linear Regression $\displaystyle\underset{\boldsymbol{\beta}(x_0)}{\text{argmin}}(\mathbf{y}-X\boldsymbol{\beta}(x_0))^TW(x_0)(\mathbf{y}-X\boldsymbol{\beta}(x_0))$
The coefficients of local linear regression can be obtained by finding a optimal solution of the problem above, where $W$ is a diagonal matrix whose $i$th diagonal elements are $K_\lambda(x_0, x_i)$."><meta itemprop="datePublished" content="2021-03-20T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-03-20T00:00:00+00:00" />
<meta itemprop="wordCount" content="564">
<meta itemprop="keywords" content="" />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GE14Z4QTKV"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-GE14Z4QTKV', { 'anonymize_ip': false });
}
</script>

</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="INTUITIVE STATISTICAL LEARNING" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">INTUITIVE STATISTICAL LEARNING</div>
					<div class="logo__tagline">Wanna be a Skillful Data Scientist</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Local Regression</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2021-03-20T00:00:00Z">2021-03-20</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/ml/" rel="category">ML</a>
	</span>
</div></div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#local-linear-regression">Local Linear Regression</a></li>
    <li><a href="#local-polynomial-regression">Local Polynomial Regression</a></li>
    <li><a href="#python-code-for-local-regression">Python Code for Local Regression</a></li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<p>Boundary issues of Nadaraya-Watson kernel regression can be solved by fitting a straight line rather than constants locally. This is the concept of local regression which fits a seperate weighted least squares at each target point $\mathbf{X}_0$. For convenience, let&rsquo;s consider the one-dimensional input space from now on.</p>
<h2 id="local-linear-regression">Local Linear Regression</h2>
<p>$\displaystyle\underset{\boldsymbol{\beta}(x_0)}{\text{argmin}}(\mathbf{y}-X\boldsymbol{\beta}(x_0))^TW(x_0)(\mathbf{y}-X\boldsymbol{\beta}(x_0))$</p>
<p>The coefficients of local linear regression can be obtained by finding a optimal solution of the problem above, where $W$ is a diagonal matrix whose $i$th diagonal elements are $K_\lambda(x_0, x_i)$. It can be done by the same way as in the linear regression.</p>
<p>$\begin{aligned}
&amp;\dfrac{\partial}{\partial\boldsymbol{\beta}(x_0)}(\mathbf{y}-X\boldsymbol{\beta}(x_0))^TW(x_0)(\mathbf{y}-X\boldsymbol{\beta}(x_0)) \\
&amp;=\dfrac{\partial}{\partial\boldsymbol{\beta}(x_0)}(\mathbf{y}^TW(x_0)\mathbf{y}-\boldsymbol{\beta}^TX^TW(x_0)\mathbf{y}-\mathbf{y}^TW(x_0)X\boldsymbol{\beta}+\boldsymbol{\beta}^TX^TW(x_0)X\boldsymbol{\beta}) \\
&amp;=-2X^TW(x_0)\mathbf{y}+2X^TW(x_0)X\boldsymbol{\beta}
\end{aligned}$</p>
<p>Without a doubt, $\hat{\boldsymbol{\beta}}(x_0)=(X^TW(x_0)X)^{-1}X^TW(x_0)\mathbf{y}$ is the solution. Now we can get $\hat{f}(x_0)$ as below.</p>
<p>$\begin{aligned}
\hat{f}(x_0)&amp;=\mathbf{x}_0^T(X^TW(x_0)X)^{-1}X^TW(x_0)\mathbf{y} \\
&amp;=\mathbf{l}(x_0)^T\mathbf{y} \\
&amp;=\sum_{i=1}^nl_i(x_0)y_i
\end{aligned}$</p>
<p>where $\mathbf{x}_0=\begin{bmatrix} 1 \\ x_0 \end{bmatrix}$. Sometimes we call $l_i(x_0)$ as equivalent kernel.</p>
<p>Local linear regression can be the alternative of Nadaraya-Watson kernel regression because it automatically reduces the bias to first order. Below is the proof for this property.</p>
<p>1) $\displaystyle\sum_{i=1}^nl_i(x_0)=1$, $\displaystyle\sum_{i=1}^nl_i(x_0)x_i=x_0$</p>
<p>We&rsquo;ve already showed that $\displaystyle\mathbf{x}_0^T(X^TW(x_0)X)^{-1}X^TW(x_0)\mathbf{y}=\sum_{i=1}^nl_i(x_0)y_i$. Now let $\mathbf{v}_j=\begin{bmatrix} x_1^j &amp; x_2^j &amp; \cdots &amp; x_n^j \end{bmatrix}^T$.</p>
<p>Then we can show that $\displaystyle\mathbf{x}_0^T(X^TW(x_0)X)^{-1}X^TW(x_0)\mathbf{v}_j=\sum_{i=1}^nl_i(x_0)x_i^j$.</p>
<p>$\begin{aligned}
\mathbf{x}_0^T(X^TW(x_0)X)^{-1}X^TW(x_0)\begin{bmatrix} \mathbf{v}_0 &amp; \mathbf{v}_1 \end{bmatrix}&amp;=\mathbf{x}_0^T(X^TW(x_0)X)^{-1}X^TW(x_0)X \\
&amp;=\mathbf{x}_0^T \\
&amp;=\begin{bmatrix} 1 &amp; x_0 \end{bmatrix}
\end{aligned}$</p>
<p>$\displaystyle\therefore \sum_{i=1}^nl_i(x_0)=1, \; \sum_{i=1}^nl_i(x_0)x_i=x_0$</p>
<p>2) $\text{Bias}(\hat{f}(x_0))=\dfrac{f&rsquo;&rsquo;(x_0)}{2}\sum_{i=1}^n(x_i-x_0)^2l_i(x_0)+R$</p>
<p>By using taylor expansion, we can write down the expectation of $\hat{f}(x_0)$ as below.</p>
<p>$\begin{aligned}
\text{E}[\hat{f}(x_0)]&amp;=\sum_{i=1}^nl_i(x_0)f(x_i) \\
&amp;=f(x_0)\sum_{i=1}^nl_i(x_0)+f&rsquo;(x_0)\sum_{i=1}^n(x_i-x_0)l_i(x_0)+\dfrac{f&rsquo;&rsquo;(x_0)}{2}\sum_{i=1}^n(x_i-x_0)^2l_i(x_0)+R
\end{aligned}$</p>
<p>We showed that $\displaystyle\sum_{i=1}^nl_i(x_0)=1$ and $\displaystyle\sum_{i=1}^nl_i(x_0)x_i=x_0$.</p>
<p>$\displaystyle\sum_{i=1}^n(x_i-x_0)l_i(x_0)=\sum_{i=1}^nx_il_i(x_0)-x_0\sum_{i=1}^nl_i(x_0)=0$</p>
<p>$\displaystyle\therefore\text{E}[\hat{f}(x_0)]=f(x_0)+\dfrac{f&rsquo;&rsquo;(x_0)}{2}\sum_{i=1}^n(x_i-x_0)^2l_i(x_0)+R$</p>
<p>This implies that the bias of $\hat{f}(x_0)$ only depends on the second derivative and the higher-order terms.</p>
<h2 id="local-polynomial-regression">Local Polynomial Regression</h2>
<p>The only difference between local linear regression and local polynomial regression is the maximum degree of the model. We just substitute $X$ with $\begin{bmatrix} \mathbf{1} &amp; \mathbf{x} &amp; \mathbf{x}^2 &amp; \cdots &amp; \mathbf{x}^d \end{bmatrix}$. Then the bias of $d$th-order local polynomial only depends on the $(d+1)$th derivative and the higher-order terms. We can prove this by the similar way as in the local linear.</p>
<p>Usually local linear fits are useful to dramatically decrease the bias at the boundaries, while local quadratic fits tend to be most helpful in reducing bias due to curvature in the interior of the domain.</p>
<p>The benefit of automatic kernel carpentry comes out as the dimension gets highger. A tendency of data getting closer to the boundary makes the asymmetry problem more serious, but local regression can take care of it. However, if the dimension of the input space becomes larger than three, local regression becomes less useful as the range for a neighborhood gets larger.</p>
<h2 id="python-code-for-local-regression">Python Code for Local Regression</h2>
<p>Github Link: <a href="https://github.com/statkwon/ML_Study/blob/master/MyLocalRegression.ipynb">MyLocalRegression.ipynb</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MyLocalRegression</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Tri-Cube&#39;</span>, width<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>kernel <span style="color:#f92672">=</span> kernel
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>width <span style="color:#f92672">=</span> width
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tricube</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(abs(x) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>, (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>abs(x)<span style="color:#f92672">**</span><span style="color:#ae81ff">3</span>)<span style="color:#f92672">**</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">epanechnikov</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(abs(x) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0.75</span><span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>), <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gaussian</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>math<span style="color:#f92672">.</span>pi)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>(x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X_train, y_train, X_test):
</span></span><span style="display:flex;"><span>        X_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(X_train)
</span></span><span style="display:flex;"><span>        y_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(y_train)
</span></span><span style="display:flex;"><span>        X_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(X_test)
</span></span><span style="display:flex;"><span>        y_pred <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X_test)):
</span></span><span style="display:flex;"><span>            t <span style="color:#f92672">=</span> abs(X_train<span style="color:#f92672">-</span>X_test[i])<span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>width
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>kernel <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;Tri-Cube&#39;</span>:
</span></span><span style="display:flex;"><span>                d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>tricube(t)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">elif</span> self<span style="color:#f92672">.</span>kernel <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;Epanechnikov&#39;</span>:
</span></span><span style="display:flex;"><span>                d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>epanechnikov(t)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gaussian(t)
</span></span><span style="display:flex;"><span>            X_train_nonzero <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>column_stack((np<span style="color:#f92672">.</span>power(X_train[d <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">0</span>), X_train[d <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>]))
</span></span><span style="display:flex;"><span>            y_train_nonzero <span style="color:#f92672">=</span> y_train[d <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>            W <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>diag(d[d <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>            y_pred <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(y_pred, np<span style="color:#f92672">.</span>transpose(np<span style="color:#f92672">.</span>array((<span style="color:#ae81ff">1</span>, X_test[i])))<span style="color:#f92672">.</span>dot(np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(np<span style="color:#f92672">.</span>transpose(X_train_nonzero)<span style="color:#f92672">.</span>dot(W)<span style="color:#f92672">.</span>dot(X_train_nonzero)))<span style="color:#f92672">.</span>dot(np<span style="color:#f92672">.</span>transpose(X_train_nonzero))<span style="color:#f92672">.</span>dot(W)<span style="color:#f92672">.</span>dot(y_train_nonzero))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y_pred
</span></span></code></pre></div><hr>
<p><strong>Reference</strong></p>
<ol>
<li>Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.</li>
</ol>

		</div>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="STATKWON avatar" src="/img/avatar.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About STATKWON</span>
	</div>
	<div class="authorbox__description">
		Wanna be a Skillful Data Scientist
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/ml/kernel_regression/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Kernel Regression</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/ml/perceptron_learning_algorithm/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Perceptron Learning Algorithm</p>
		</a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statkwon" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 STATKWON.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\[','\]']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: { equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"] }
        }
    });
    MathJax.Hub.Queue(function() {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

</body>
</html>