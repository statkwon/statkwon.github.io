<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mls on ML LAB</title>
    <link>http://statkwon.github.io/ml/</link>
    <description>Recent content in Mls on ML LAB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://statkwon.github.io/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Maximum Likelihood Estimation</title>
      <link>http://statkwon.github.io/ml/maximum_likelihood_estimation/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/maximum_likelihood_estimation/</guid>
      <description>MLE는 통계학에서 모수를 추정하기 위한 방법 중 하나이다. 말 그대로 likelihood를 최대화하는 추정치를 사용하여 모수를 추정하는 방식이다.
likelihood는 모수의 함수이다. 즉, $L(\theta)$는 데이터가 주어졌을 때 $f(x;\theta)$로부터 해당 데이터가 sampling 되었을 가능성을 의미한다.
따라서 데이터 $x_1, \ldots, x_n$이 동일한 분포로부터 independently sampling 되었다면, $L(\theta)=\prod_{i=1}^nf(x_i;\theta)$가 된다.
MLE는 다음과 같은 좋은 성질을 갖는다.
consistency: $\hat{\theta}_n\overset{p}{\rightarrow}\theta$ asymptotic normality: $\sqrt{n}(\hat{\theta}_n-\theta)\overset{d}{\rightarrow}N(0, \theta^2)$ MLE의 분산은 Cramér-Rao lower bound와 근사적으로 같다. </description>
    </item>
    
    <item>
      <title>Monte-Carlo &amp; Bootstrap</title>
      <link>http://statkwon.github.io/ml/monte-carlo_and_bootstrap/</link>
      <pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/monte-carlo_and_bootstrap/</guid>
      <description>Monte-Carlo Method Monte-Carlo 방법은 random sampling을 통해 풀고자 하는 문제에 대한 numerical result를 얻는 방식이다.
예를 들어, $X\sim\text{Unif}(2, 4)$일 때, $\mathbb{E}[X]$의 analytical한 해는 다음과 같다.
$\displaystyle \mathbb{E}[X]=\int_2^4x\cdot\dfrac{1}{2}dx=3$
만약 closed-form으로 해를 구할 수 없는 경우라면 $\text{Unif}(2, 4)$로부터의 random sampling을 통해 numerical한 해를 구할 수 있다.
np.random.seed(0) X = np.random.uniform(2, 4, size=1000) np.mean(X) # 2.9918430687435653 Bootstrap Method Bootstrap은 데이터를 사용하여 sampling distribution을 근사하는 방식이다.
Parametric Bootstrap은 데이터의 분포는 알지만 모수는 알지 못하는 경우 모수의 추정치를 사용하여 sampling distribution을 근사한다.</description>
    </item>
    
    <item>
      <title>Standardizing &amp; Whitening</title>
      <link>http://statkwon.github.io/ml/standardizing_and_whitening/</link>
      <pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/standardizing_and_whitening/</guid>
      <description>Random Vector 관점 Standardizing(Normalization)은 mean이 $\boldsymbol{\mu}$이고 covariance가 $\Sigma$인 분포를 따르는 random vector $\mathbf{X}$의 분포가 mean이 $\mathbf{0}$이고 각 feature의 variance가 $1$인 분포가 되도록 처리하는 것 → 각 feature 별 연산이므로 feature 간 covariance에는 영향이 없다.
Whitening은 mean이 $\mathbf{0}$이고 covariance가 $\Sigma$인 분포를 따르는 random vector $\mathbf{X}$의 covariance가 identity matrix가 되도록 처리하는 것 → feature 간 covariance를 $0$으로 만들어 줄 수 있다.
$\mathbf{Y}=W\mathbf{X}$일 때, $\text{Cov}(\mathbf{Y})=W\text{Cov}(\mathbf{X})W^T=W\Sigma W^T$
$W\Sigma W^T$가 $I$가 되게 하려면 $W$가 $W^TW=\Sigma^{-1}$를 만족하도록 하면 된다.</description>
    </item>
    
  </channel>
</rss>
