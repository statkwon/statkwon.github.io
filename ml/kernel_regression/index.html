<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Kernel Regression - ML LAB</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Kernel Regression" />
<meta property="og:description" content="Kernel smoothing is a meomry-based method to add a flexibility in estimating the regression function by fitting a different but simple model seperately at each query point $x_0$ with only those observations close to the target point $x_0$. This localization is achieved via kernel $K_\lambda(\mathbf{X}_0, \mathbf{X})$, which assigns a weight to $\mathbf{X}$ based on its distance from $\mathbf{X}_0$.
Kernel Functions $K_\lambda(\mathbf{X}_0, \mathbf{X})=D\left(\dfrac{d(\mathbf{X}_0, \mathbf{X})}{h_\lambda(\mathbf{X}_0)}\right)$
Kernel function can be divided by three parts: distance function, width function, and weighting function." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://statkwon.github.io/ml/kernel_regression/" /><meta property="article:section" content="ml" />
<meta property="article:published_time" content="2021-03-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-03-17T00:00:00+00:00" />

		<meta itemprop="name" content="Kernel Regression">
<meta itemprop="description" content="Kernel smoothing is a meomry-based method to add a flexibility in estimating the regression function by fitting a different but simple model seperately at each query point $x_0$ with only those observations close to the target point $x_0$. This localization is achieved via kernel $K_\lambda(\mathbf{X}_0, \mathbf{X})$, which assigns a weight to $\mathbf{X}$ based on its distance from $\mathbf{X}_0$.
Kernel Functions $K_\lambda(\mathbf{X}_0, \mathbf{X})=D\left(\dfrac{d(\mathbf{X}_0, \mathbf{X})}{h_\lambda(\mathbf{X}_0)}\right)$
Kernel function can be divided by three parts: distance function, width function, and weighting function."><meta itemprop="datePublished" content="2021-03-17T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-03-17T00:00:00+00:00" />
<meta itemprop="wordCount" content="648">
<meta itemprop="keywords" content="Kernel," />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GE14Z4QTKV"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-GE14Z4QTKV', { 'anonymize_ip': false });
}
</script>

</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="INTUITIVE STATISTICAL LEARNING" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">INTUITIVE STATISTICAL LEARNING</div>
					<div class="logo__tagline">Wanna be a Skillful ML Engineer</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Kernel Regression</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2021-03-17T00:00:00Z">2021-03-17</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/ml/" rel="category">ML</a>
	</span>
</div></div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#kernel-functions">Kernel Functions</a></li>
    <li><a href="#nadaraya-watson-kernel-regression">Nadaraya-Watson Kernel Regression</a></li>
    <li><a href="#python-code-for-nadaraya-watson-kernel-regression">Python Code for Nadaraya-Watson Kernel Regression</a></li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<p>Kernel smoothing is a meomry-based method to add a flexibility in estimating the regression function by fitting a different but simple model seperately at each query point $x_0$ with only those observations close to the target point $x_0$. This localization is achieved via kernel $K_\lambda(\mathbf{X}_0, \mathbf{X})$, which assigns a weight to $\mathbf{X}$ based on its distance from $\mathbf{X}_0$.</p>
<h2 id="kernel-functions">Kernel Functions</h2>
<p>$K_\lambda(\mathbf{X}_0, \mathbf{X})=D\left(\dfrac{d(\mathbf{X}_0, \mathbf{X})}{h_\lambda(\mathbf{X}_0)}\right)$</p>
<p>Kernel function can be divided by three parts: distance function, width function, and weighting function. A few famous distances such as euclidean distance or mahalanobis distance can be selected for the distance function. There are two types of width, metric window width $h_\lambda(\mathbf{X}_0)=\lambda$, which is just a constant and nearest-neighbor window width $h_k(\mathbf{X}_0)=d(\mathbf{X}_0, \mathbf{X}_{[k]})$, where $\mathbf{X}_{[k]}$ is the $k$th closest point to $\mathbf{X}_0$. Lastly, there exist several candidates of weighting function $D(t)$. We will only cover about some of them below.</p>
<p>1) Epanechnikov: $D(t)=\begin{cases} \dfrac{3}{4}(1-t)^2 &amp; \vert t\vert\leq1 \\ 0 &amp; \text{o.w.} \end{cases}$</p>
<p>2) Tri-Cube: $D(t)=\begin{cases} (1-\vert t\vert^3)^3 &amp; \vert t\vert\leq1 \\ 0 &amp; \text{o.w.} \end{cases}$</p>
<p>3) Gaussian: $D(t)=\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}t^2}, \quad -\infty&lt;t&lt;\infty$</p>
<p>4) Uniform: $D(t)=\begin{cases} \dfrac{1}{2} &amp; \vert t\vert\leq1 \\ 0 &amp; \text{o.w.} \end{cases}$</p>
<h2 id="nadaraya-watson-kernel-regression">Nadaraya-Watson Kernel Regression</h2>
<p>Nadaraya-Watson kernel regression estimates $\hat{f}(\mathbf{X}_0)$ with a weighted average of the observations close to $\mathbf{X}_0$. Here the weights are determined by the distances of the observations from $\mathbf{X}_0$.</p>
<p>$\hat{f}(\mathbf{X}_0)=\dfrac{\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)y_i}{\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)}$</p>
<p>This can be interpreted as using the kernel density estimation for $f(\mathbf{X}_0)$ and $f(\mathbf{X}_0, y_0)$ to estimate $\text{E}[Y_0\vert\mathbf{X}_0]$.</p>
<p>$\begin{aligned}
\text{E}[Y_0\vert\mathbf{X}_0]&amp;=\int y_0\dfrac{f(\mathbf{X}_0, y)}{f(\mathbf{X}_0)}dy_0 \\
&amp;=\int\dfrac{y_0\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)K_\lambda(y_0, y_i)}{\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)}dy_0 \\
&amp;=\dfrac{\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)\int y_0K_\lambda(y_0, y_i)dy_0}{\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)} \\
&amp;=\dfrac{\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)y_i}{\sum_{i=1}^nK_\lambda(\mathbf{X}_0, \mathbf{X}_i)}
\end{aligned}$</p>
<p>The continuity of $\hat{f}(\mathbf{X})$ depends on which kernel to use. If we use the continuous kernel like epanechnikov kernel, the result curve will be smooth, while the result for the discontinuous kernel like nearest-neighbor kernel is rigid.</p>
<p>We need to select an appropriate value for $\lambda$(or $k$) because it has a decisive effect on the bias and variance of the model. As $\lambda$ becomes larger, the variance of the model gets lower, while the bias gets higher. Thus, we call $\lambda$ as a smoothing parameter.</p>
<p>Usually metric window width tends to keep the bias of the estimate constant, but the variance is inversely proportional to the local density. Nearest-neighbor window width exhibits the opposite behavior; the variance stays constant and the absolute biase varies inversely with local density. These are intuitive results because the number of points included in the metric window is proportional to the local density, while the length of nearest-neighbor window is inversely proportional to the local density.</p>
<p>However, both methods suffers from boundary issues. Local density around the boundaries is naturally low and this makes the metric window to contain less points and the nearest-neighbor window to get wider. Also, locally weighted averages can be badly biased on the boundaries of the domain, because of the asymmetry of the kernel in that region. This bias can also occur in the interior of the domain, if the $\mathbf{X}$ values are not equally spaced, but is usually less severe.</p>
<h2 id="python-code-for-nadaraya-watson-kernel-regression">Python Code for Nadaraya-Watson Kernel Regression</h2>
<p>Github Link: <a href="https://github.com/statkwon/ML_Study/blob/master/MyNWKernelRegression.ipynb">MyNWKernelRegression.ipynb</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MyNWKernelRegression</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Epanechnikov&#39;</span>, width<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>kernel <span style="color:#f92672">=</span> kernel
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>width <span style="color:#f92672">=</span> width
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">epanechnikov</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(abs(x) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0.75</span><span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>), <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tricube</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(abs(x) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>, (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>abs(x)<span style="color:#f92672">**</span><span style="color:#ae81ff">3</span>)<span style="color:#f92672">**</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gaussian</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>math<span style="color:#f92672">.</span>pi)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>(x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">uniform</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(abs(x) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X_train, y_train, X_test):
</span></span><span style="display:flex;"><span>        X_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(X_train)
</span></span><span style="display:flex;"><span>        y_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(y_train)
</span></span><span style="display:flex;"><span>        X_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(X_test)
</span></span><span style="display:flex;"><span>        y_pred <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X_test)):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>kernel <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;KNN&#39;</span>:
</span></span><span style="display:flex;"><span>                t <span style="color:#f92672">=</span> abs(X_train<span style="color:#f92672">-</span>X_test[i])<span style="color:#f92672">/</span>abs(X_train<span style="color:#f92672">-</span>X_test[i])[np<span style="color:#f92672">.</span>argsort(abs(X_train<span style="color:#f92672">-</span>X_test[i]))<span style="color:#f92672">==</span>self<span style="color:#f92672">.</span>width][<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>                d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>uniform(t)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                t <span style="color:#f92672">=</span> abs(X_train<span style="color:#f92672">-</span>X_test[i])<span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>width
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>kernel <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;Epanechnikov&#39;</span>:
</span></span><span style="display:flex;"><span>                    d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>epanechnikov(t)
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">elif</span> self<span style="color:#f92672">.</span>kernel <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;Tri-Cube&#39;</span>:
</span></span><span style="display:flex;"><span>                    d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>tricube(t)
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                    d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gaussian(t)
</span></span><span style="display:flex;"><span>            y_pred <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(y_pred, np<span style="color:#f92672">.</span>sum(d<span style="color:#f92672">*</span>y_train)<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sum(d))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y_pred
</span></span></code></pre></div><hr>
<p><strong>Reference</strong></p>
<ol>
<li>Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.</li>
<li><a href="https://en.wikipedia.org/wiki/Kernel_regression">https://en.wikipedia.org/wiki/Kernel_regression</a></li>
</ol>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/kernel/" rel="tag">Kernel</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="STATKWON avatar" src="/img/avatar.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About STATKWON</span>
	</div>
	<div class="authorbox__description">
		Wanna be a Skillful ML Engineer
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/ml/smoothing_spline/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Smoothing Spline</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/ml/local_regression/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Local Regression</p>
		</a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statkwon" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 STATKWON.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\[','\]']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: { equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"] }
        }
    });
    MathJax.Hub.Queue(function() {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

</body>
</html>