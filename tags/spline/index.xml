<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spline on ML LAB</title>
    <link>http://statkwon.github.io/tags/spline/</link>
    <description>Recent content in Spline on ML LAB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://statkwon.github.io/tags/spline/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MARS</title>
      <link>http://statkwon.github.io/ml/mars/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/mars/</guid>
      <description>Multivariate Adaptive Regression Splines $\displaystyle f(\mathbf{x})=\beta_0+\sum_{m=1}^M\beta_mh_m(\mathbf{x})$, where $h_m(\mathbf{x})=(X_j-t)_+\;\text{or}\; (t-X_j)_+$
MARS는 다음과 같은 특수한 형태의 Basis Function들의 선형 결합으로 반응변수를 추정하는 알고리즘이다.
$(x-t)_+=\begin{cases} x-t &amp;amp; \text{if}\; x&amp;gt;t \\ 0 &amp;amp; \text{otherwise} \end{cases} \quad\text{and}\quad (t-x)_+=\begin{cases} t-x &amp;amp; \text{if}\; x&amp;lt;t \\ 0 &amp;amp; \text{otherwise} \end{cases}$
MARS는 Forward Modeling과 Backward Deletion의 두 가지 단계로 구성되어 있다.
Forward Modeling $(x-t)_+$와 $(t-x)_+$가 서로 대칭적인 구조를 가지므로 두 함수를 묶어 Reflected Pair라고 하자. Forward Modeling 단계에서는 이러한 Reflected Pair들의 집합</description>
    </item>
    
    <item>
      <title>PRIM</title>
      <link>http://statkwon.github.io/ml/prim/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/prim/</guid>
      <description>Patient Rule Induction Method PRIM은 CART 알고리즘과 같이 Feature Space를 박스 형태의 영역들로 분할하는 알고리즘이다. 하지만 Binary Split을 사용하지는 않는다. CART 알고리즘이 Splitting과 Pruning의 두 가지 단계로 이루어졌듯이, PRIM 알고리즘은 Top-Down Peeling과 Bottom-Up Pasting의 두 가지 단계로 구분된다.
Top-Down Peeling 모든 데이터가 한 박스 안에 담겨있다고 생각해보자. 박스의 한 면을 선택하고 전체 데이터 중 비율 $\alpha$ 만큼의 데이터가 제외될 때까지 선택된 면을 기준으로 박스의 크기를 줄인다. 이때 줄어든 박스에 속한 데이터의 반응변수의 평균을 최대화하는 것을 기준으로 면을 선택한다.</description>
    </item>
    
    <item>
      <title>Smoothing Spline</title>
      <link>http://statkwon.github.io/ml/smoothing_spline/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/smoothing_spline/</guid>
      <description>Without any constraint on the form of $f(X)$, we can make the $\text{RSS}$ to be $0$ by choosing any function that interpolates all data points. However, this will be connected to an overfitting problem. To prevent this, we will use the regularization term.
$$\underset{f}{\text{argmin}}\sum_{i=1}^n\left\{y_i-f(x_i)\right\}^2+\lambda\int\left\{f^{\left(\frac{M+1}{2}\right)}(x)\right\}^2dx$$
Here $\lambda$ is called a smoothing parameter. If $\lambda=0$, $f$ can be any function that interpolates the data. If $\lambda=\infty$, $f$ will be a simple line. Now we will show that the unique minimizer of this criterion is a natural cubic spline with knots at each of the $x_i$.</description>
    </item>
    
    <item>
      <title>Natural Spline</title>
      <link>http://statkwon.github.io/ml/natural_spline/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/natural_spline/</guid>
      <description>Adding another constraint to regression spline, we can fit natural splines. We will reduce the degree beyond the boundary to $\dfrac{M-1}{2}$.
The number of parameters to fit natural splines is $(M+1)\times(K-1)-\left(\dfrac{M-1}{2}+1\right)\times2-M\times K=K$ and we can see that it is independent of $M$.
Natural Cubic Spline is the most common one.
Natural Cubic Spline $N_1(X)=1, \quad N_2(X)=X, \quad N_{k+2}(X)=d_k(X)-d_{K-1}(X)$
$d_k(X)=\dfrac{(X-\xi_k)_+^3-(X-\xi_K)_+^3}{\xi_k-\xi_K}$
We will make a proof for the formula above.
$\displaystyle f(X)=\sum_{j=1}^4\beta_jX^{j-1}+\sum_{k=1}^K\theta_k(X-\xi_k)_+^3$</description>
    </item>
    
    <item>
      <title>Regression Spline</title>
      <link>http://statkwon.github.io/ml/regression_spline/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/regression_spline/</guid>
      <description>We can obtain regression splines by adding a continuity constraint to piecewise polynomials.
Regression splines are often called the $M$th order spline, a piecewise polynomial of degree $M$, that is continuous and has continuous derivatives of orders $1, \ldots, M-1$ at its know points.
$M$th order spline with $K$ knots $h_j(X)=X^{j-1} \quad (j=1, 2, \ldots, M+1)$
$h_{M+1+l}(X)=(X-\xi_l)_+^M \quad (l=1, 2, \ldots, K)$
$f(X)=\beta_1+\beta_2X\cdots+\beta_{M+1}X^M+\beta_{M+2}(X-\xi_1)_+^M+\cdots+\beta_{M+K+1}(X-\xi_K)_+^M$
We will make a proof for the case when $M=3$ and $K=2$.</description>
    </item>
    
  </channel>
</rss>
