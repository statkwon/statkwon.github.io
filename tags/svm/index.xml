<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SVM on ML LAB</title>
    <link>http://statkwon.github.io/tags/svm/</link>
    <description>Recent content in SVM on ML LAB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 23 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://statkwon.github.io/tags/svm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Support Vector Classifier</title>
      <link>http://statkwon.github.io/ml/support_vector_classifier/</link>
      <pubDate>Sun, 23 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/support_vector_classifier/</guid>
      <description>Support Vector Classifier는 Optimal Seperating Hyperplane을 Nonseperable Case에 대해 일반화한 모형이다. Support Vector Classifier 역시 Margin을 최대화하는 방향으로 작동하지만, 일정 수준의 오분류를 허용함으로써 Nonseperable Case에서도 수렴할 수 있다는 것이 차이점이다.
일정 수준의 오분류를 허용한다는 것을 &amp;lsquo;Slack Variable&amp;rsquo;라고 불리는 $\xi_i$를 사용하여 다음과 같이 표현할 수 있다. (위 그림에서 $\xi_i^*=M\xi_i$이다.)
$\displaystyle \max_{\boldsymbol{\beta}, \beta_0, \Vert\boldsymbol{\beta}\Vert=1}M \quad\text{subject to}\quad y_i(\mathbf{x}_i^T\boldsymbol{\beta}+\beta_0)\geq M-\xi_i,\; \xi_i\geq 0,\; \sum_{i=1}^N\xi_i\leq c,\; ^\forall i$
하지만 이러한 형태의 제약 조건을 사용할 경우, 더 이상 주어진 문제가 Convex Optimization에 속하지 않는다.</description>
    </item>
    
    <item>
      <title>Optimal Seperating Hyperplanes</title>
      <link>http://statkwon.github.io/ml/optimal_seperating_hyperplanes/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>http://statkwon.github.io/ml/optimal_seperating_hyperplanes/</guid>
      <description>Optimal Seperating Hyperplane은 Perceptron Algorithm의 해가 유일하지 않다는 단점을 보완하기 위해 고안된 방식이다. Perceptron Learning Algorithm과 마찬가지로 $y\in\{-1, 1\}$의 Binary Classification 문제에 적용되지만, 데이터와 분류 경계선 사이의 빈 공간을 뜻하는 &amp;lsquo;Margin&amp;rsquo;이라는 새로운 개념을 도입해다는 점에서 차이가 있다. Optimal Seperating Hyperplane의 목표는 이 공간을 최대화하는 Hyperplane을 찾는 것이다.
Thus, we have to find optimal $\boldsymbol{\beta}$ and $\beta_0$ which maximizes $M$ when the distance between each points and the boundary is greater than or equal to $M$.</description>
    </item>
    
  </channel>
</rss>
